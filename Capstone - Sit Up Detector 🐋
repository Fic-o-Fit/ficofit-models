{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Capstone - Sit Up Detector üêã","provenance":[{"file_id":"1J8E2UZoWUZcd5mp6brqSD_-gpqaWonMa","timestamp":1651865637744}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# üí™üèΩ**Sit Up Detector**üí¶‚ú®\n","\n","This notebook is presented to you by Bangkit Team C22-PS072's ML Squadüíñ:\n","\n","\n","*   Deandra Setyaputri - M2010F1120\n","*   Wilma Elysia - M7013F1348\n","\n"],"metadata":{"id":"3YSzRfpc5NnJ"}},{"cell_type":"markdown","source":["## 0. Manage Dependencies"],"metadata":{"id":"tiI0kJBbwui0"}},{"cell_type":"code","source":["!pip install tensorflow tensorflow-gpu opencv-python matplotlib sklearn"],"metadata":{"id":"KZVy7KhvXSAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import glob\n","import cv2\n","import os\n","import enum\n","import tensorflow as tf\n","from tensorflow import keras"],"metadata":{"id":"hQly5z6WydbC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Preprocess Input Images\n","First, create a copy of every image data where they are flipped horizontally so that the model can accurately detect push-up poses regardless if the user is facing left or right."],"metadata":{"id":"dspDqEBd_Vq6"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount(\"/content/drive\") \n","root = \"/content/drive/My Drive/Datasets/pushup/\"\n","\n","pushup_up_folder = root + \"pushup_up\" + \"/*.*\"\n","pushup_down_folder = root + \"pushup_down\" + \"/*.*\"\n","movenet_folder = \"/content/drive/My Drive/Models/lite-model_movenet_singlepose_lightning_3.tflite\""],"metadata":{"id":"qNz5rgwh_eZT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652009110638,"user_tz":-420,"elapsed":20191,"user":{"displayName":"Deandra Setyaputri M2010F1120","userId":"08611358996117971618"}},"outputId":"39090ce0-1aaa-4032-d50f-84ad705af1e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["def flip_img_folder(folder):\n","    for file in glob.glob(folder):\n","        image = cv2.imread(file)\n","        flipped_img_path = file[:-4] + \"-reversed.\" + file[-3:]\n","        # file[:-4] is the name of the path and image name without the .jpg or .png extension\n","\n","        if(file[-13:-4] != \"-reversed\" and os.path.exists(flipped_img_path) == False):\n","            flipped_img = cv2.flip(image, 1)\n","            cv2.imwrite(flipped_img_path, flipped_img)"],"metadata":{"id":"WmgnqaZxEKga"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["flip_img_folder(pushup_up_folder)\n","flip_img_folder(pushup_down_folder)"],"metadata":{"id":"t5R58Me4PnZm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will convert all the image data to arrays of landmarks representing the human pose in each image. Then create a csv file containing those data to be used in training later."],"metadata":{"id":"tV_qwPJfD9qX"}},{"cell_type":"code","source":["interpreter = tf.lite.Interpreter(model_path=movenet_folder)\n","interpreter.allocate_tensors()"],"metadata":{"id":"gPZK5IHId-ot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def processPose(image):\n","    img = tf.image.resize_with_pad(np.expand_dims(image, axis=0), 192,192)\n","    img = tf.cast(img, dtype=tf.float32)\n","    input_details = interpreter.get_input_details()\n","    output_details = interpreter.get_output_details()\n","    interpreter.set_tensor(input_details[0]['index'], np.array(img))\n","    # Invoke inference.\n","    interpreter.invoke()\n","    # Get the model prediction.\n","    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n","    keypoints = keypoints_with_scores.flatten().reshape(17,3)\n","    fixed_keypoints = []\n","    for [x, y, score] in keypoints:\n","        if score < 0.2:\n","            fixed_keypoints.append([0, 0, 0])\n","        else:\n","            fixed_keypoints.append([x, y, score])\n","    return np.array(fixed_keypoints)"],"metadata":{"id":"tAnE2ZRkYdVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# trying out the library\n","for file in glob.glob(pushup_up_folder):\n","    img = cv2.imread(file)\n","    keypoints = processPose(img).flatten().reshape(17, 3)\n","    \n","    break\n","img = cv2.imread('/content/800px-A_black_image.jpg')\n","keypoints = processPose(img).flatten().reshape(17, 3)\n","fixed_keypoints = []\n","for [x, y, score] in keypoints:\n","    if score < 0.2:\n","        fixed_keypoints.append([0, 0, 0])\n","    else:\n","        fixed_keypoints.append([x, y, score])\n","print(fixed_keypoints)\n","    # landmarks = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_world_landmarks.landmark]).flatten()\n","    # print(landmarks)"],"metadata":{"id":"2770Ruv2SKiD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652020395774,"user_tz":-420,"elapsed":356,"user":{"displayName":"Deandra Setyaputri M2010F1120","userId":"08611358996117971618"}},"outputId":"2e34e6b7-95b1-485b-dbe3-b1d16e52daec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n"]}]},{"cell_type":"code","source":["# Source: the Movenet TFLite Documentation\n","class BodyPart(enum.Enum):\n","  \"\"\"Enum representing human body keypoints detected by pose estimation models.\"\"\"\n","  NOSE = 0\n","  LEFT_EYE = 1\n","  RIGHT_EYE = 2\n","  LEFT_EAR = 3\n","  RIGHT_EAR = 4\n","  LEFT_SHOULDER = 5\n","  RIGHT_SHOULDER = 6\n","  LEFT_ELBOW = 7\n","  RIGHT_ELBOW = 8\n","  LEFT_WRIST = 9\n","  RIGHT_WRIST = 10\n","  LEFT_HIP = 11\n","  RIGHT_HIP = 12\n","  LEFT_KNEE = 13\n","  RIGHT_KNEE = 14\n","  LEFT_ANKLE = 15\n","  RIGHT_ANKLE = 16"],"metadata":{"id":"Nmph4npajY0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgQMdfeT65Z5"},"outputs":[],"source":["# These are a set of functions to preprocess the input data before classification\n","# Source: the Movenet TFLite Documentation\n","\n","def get_center_point(landmarks, left_bodypart, right_bodypart):\n","  \"\"\"Calculates the center point of the two given landmarks.\"\"\"\n","\n","  left = tf.gather(landmarks, left_bodypart.value, axis=1)\n","  right = tf.gather(landmarks, right_bodypart.value, axis=1)\n","  center = left * 0.5 + right * 0.5\n","  return center\n","\n","\n","def get_pose_size(landmarks, torso_size_multiplier=2.5):\n","  \"\"\"Calculates pose size.\n","\n","  It is the maximum of two values:\n","    * Torso size multiplied by `torso_size_multiplier`\n","    * Maximum distance from pose center to any pose landmark\n","  \"\"\"\n","  # Hips center\n","  hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n","                                 BodyPart.RIGHT_HIP)\n","\n","  # Shoulders center\n","  shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,\n","                                      BodyPart.RIGHT_SHOULDER)\n","\n","  # Torso size as the minimum body size\n","  torso_size = tf.linalg.norm(shoulders_center - hips_center)\n","\n","  # Pose center\n","  pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP, \n","                                     BodyPart.RIGHT_HIP)\n","  pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n","  # Broadcast the pose center to the same size as the landmark vector to\n","  # perform substraction\n","  pose_center_new = tf.broadcast_to(pose_center_new,\n","                                    [tf.size(landmarks) // (17*2), 17, 2])\n","\n","  # Dist to pose center\n","  d = tf.gather(landmarks - pose_center_new, 0, axis=0,\n","                name=\"dist_to_pose_center\")\n","  # Max dist to pose center\n","  max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n","\n","  # Normalize scale\n","  pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n","\n","  return pose_size\n","\n","\n","def normalize_pose_landmarks(landmarks):\n","  \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n","  scaling it to a constant pose size.\n","  \"\"\"\n","  # Move landmarks so that the pose center becomes (0,0)\n","  pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n","                                 BodyPart.RIGHT_HIP)\n","  pose_center = tf.expand_dims(pose_center, axis=1)\n","  # Broadcast the pose center to the same size as the landmark vector to perform\n","  # substraction\n","  pose_center = tf.broadcast_to(pose_center, \n","                                [tf.size(landmarks) // (17*2), 17, 2])\n","  landmarks = landmarks - pose_center\n","\n","  # Scale the landmarks to a constant pose size\n","  pose_size = get_pose_size(landmarks)\n","  landmarks /= pose_size\n","\n","  return landmarks\n","\n","\n","def landmarks_to_embedding(landmarks_and_scores):\n","  \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n","  # Reshape the flat input into a matrix with shape=(17, 3)\n","  reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n","\n","  # Normalize landmarks 2D\n","  landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :2])\n","\n","  # Flatten the normalized landmark coordinates into a vector\n","  embedding = keras.layers.Flatten()(landmarks)\n","\n","  return embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Pte6b1bgWKv"},"outputs":[],"source":["# Define the model\n","inputs = tf.keras.Input(shape=(51))\n","embedding = landmarks_to_embedding(inputs)\n","\n","layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)\n","layer = keras.layers.Dropout(0.5)(layer)\n","layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n","layer = keras.layers.Dropout(0.5)(layer)\n","outputs = keras.layers.Dense(2, activation=\"softmax\")(layer)\n","\n","model = keras.Model(inputs, outputs, name=\"pushupdetector\")\n","model.summary()"]},{"cell_type":"code","source":["def detect_pose_from_folder(folder, folder_label):\n","    landmarks = []\n","    labels = []\n","    for file in glob.glob(folder):\n","        image = cv2.imread(file)\n","        keypoints = processPose(image).flatten()\n","        if len(keypoints)!=0:\n","            landmarks.append(keypoints)\n","            labels.append(folder_label)\n","    return landmarks, labels"],"metadata":{"id":"9EXF5IfUhpG8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# '1' is the label for the up pushup movement\n","pushup_up_landmarks, pushup_up_labels = detect_pose_from_folder(pushup_up_folder, 1)\n","pushup_down_landmarks, pushup_down_labels = detect_pose_from_folder(pushup_down_folder, 0)"],"metadata":{"id":"FpKtOMcMZle5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["landmarks = pushup_up_landmarks + pushup_down_landmarks\n","labels = pushup_up_labels + pushup_down_labels"],"metadata":{"id":"B4lovPOAevDy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(landmarks))\n","print(len(labels))\n","print(landmarks)"],"metadata":{"id":"1jh-UB9tg9uz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","X = np.array(landmarks)\n","y = np.array(labels)\n","dataset = [(X[i], y[i]) for i in range(0, len(labels))]\n","random.shuffle(dataset)\n","X.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCrpQ0hbqs6Q","executionInfo":{"status":"ok","timestamp":1652018236768,"user_tz":-420,"elapsed":357,"user":{"displayName":"Deandra Setyaputri M2010F1120","userId":"08611358996117971618"}},"outputId":"b2155cb0-5ccd-4f29-8000-42548c67a2f0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(378, 51)"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["import csv\n","\n","# open the file in the write mode\n","with open('/content/drive/My Drive/Datasets/pushup/pushup_dataset_movenet', 'w') as f:\n","    # create the csv writer\n","    writer = csv.writer(f)\n","\n","    # write the header to the csv file\n","    writer.writerow(['pushup_pose', 'pose_world_landmarks'])\n","\n","    for landmark, label in dataset:\n","        writer.writerow([label, landmark])"],"metadata":{"id":"tgqCaUGSa5nv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Train Model"],"metadata":{"id":"SCvb0Duits5j"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5)"],"metadata":{"id":"kdtSr2kPwoEr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = keras.Sequential([\n","#         keras.layers.Input(shape=(64)),\n","#         keras.layers.Dense(128, activation='relu'),\n","#         keras.layers.Dropout(0.5),\n","#         keras.layers.Dense(64, activation='relu'),\n","#         keras.layers.Dropout(0.5),\n","#         keras.layers.Dense(2, activation=\"sigmoid\")\n","# ])\n","\n","model.summary()"],"metadata":{"id":"vV1wBCrfuH6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(\n","    optimizer='adam',\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","history = model.fit(X_train, y_train,\n","                    epochs=100,\n","                    batch_size=16,\n","                    validation_data=(X_val, y_val))"],"metadata":{"id":"K8I9vzbs2uC9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Evaluate on test data\")\n","results = model.evaluate(X_test, y_test, batch_size=32)\n","print(\"test loss, test acc:\", results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YOyDDFql4YkD","executionInfo":{"status":"ok","timestamp":1652018441318,"user_tz":-420,"elapsed":353,"user":{"displayName":"Deandra Setyaputri M2010F1120","userId":"08611358996117971618"}},"outputId":"693b3f53-e6ad-47cc-b10b-f6872cfeafb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluate on test data\n","2/2 [==============================] - 0s 5ms/step - loss: 0.2047 - accuracy: 0.9298\n","test loss, test acc: [0.2047211229801178, 0.9298245906829834]\n"]}]},{"cell_type":"code","source":["# test the model prediction\n","\n","for file in glob.glob(pushup_up_folder)[:3]:\n","    image = cv2.imread(file)\n","    keypoints = processPose(image).flatten().reshape(1, 51)\n","    predd = model.predict(keypoints)\n","    posee = np.argmax(predd)\n","    confidence = predd[0][posee]\n","    print(predd)\n","    print(posee)\n","    print(confidence)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4IVTqFkMECuG","executionInfo":{"status":"ok","timestamp":1652021143177,"user_tz":-420,"elapsed":876,"user":{"displayName":"Deandra Setyaputri M2010F1120","userId":"08611358996117971618"}},"outputId":"7b347a90-7e61-4b35-856b-549aacd0bffa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[2.656579e-04 9.997175e-01]]\n","1\n","0.9997175\n","[[1.6873252e-06 9.9999791e-01]]\n","1\n","0.9999979\n","[[9.3793496e-05 9.9990046e-01]]\n","1\n","0.99990046\n"]}]},{"cell_type":"markdown","source":["## 3. Save The Model"],"metadata":{"id":"rSBstkcD_nva"}},{"cell_type":"code","source":["!pip install pyyaml h5py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhOe3RvD_sz6","executionInfo":{"status":"ok","timestamp":1652018569257,"user_tz":-420,"elapsed":4775,"user":{"displayName":"Deandra Setyaputri M2010F1120","userId":"08611358996117971618"}},"outputId":"2763135e-2652-4863-adf1-4802e20da856"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"]}]},{"cell_type":"code","source":["model.save('pushup-counter-movenet.h5')"],"metadata":{"id":"DdM8p7nWBmDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Convert to TFlite"],"metadata":{"id":"dUEal5PAiwsW"}},{"cell_type":"code","source":["converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","tflite_model = converter.convert()\n","\n","with open('model-movenet.tflite', 'wb') as f:\n","  f.write(tflite_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8bGSSWYi5PH","executionInfo":{"status":"ok","timestamp":1652018602286,"user_tz":-420,"elapsed":8675,"user":{"displayName":"Deandra Setyaputri M2010F1120","userId":"08611358996117971618"}},"outputId":"a71b7ce8-59c0-4aaa-c62e-0d58de3c997e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Assets written to: /tmp/tmprwxutj_u/assets\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"]}]}]}